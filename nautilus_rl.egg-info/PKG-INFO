Metadata-Version: 2.4
Name: nautilus-rl
Version: 0.1.0
Summary: RL
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch
Requires-Dist: gymnasium
Requires-Dist: numpy
Requires-Dist: tqdm
Requires-Dist: tensorboard
Requires-Dist: pydantic>=2
Requires-Dist: wandb
Requires-Dist: pygame
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: ruff; extra == "dev"
Requires-Dist: mypy; extra == "dev"
Requires-Dist: pre-commit; extra == "dev"
Provides-Extra: jax
Requires-Dist: jax[cpu]; extra == "jax"
Requires-Dist: jaxlib; extra == "jax"
Requires-Dist: chex; extra == "jax"
Requires-Dist: flax; extra == "jax"
Requires-Dist: optax; extra == "jax"
Dynamic: license-file

# ğŸ§­ Nautilus â€” Reinforcement Learning Examples

**Nautilus** is a reinforcement learning (RL) codebase.

---

## ğŸš€ Quickstart

### Using Conda (Recommended for PyTorch/CUDA)

Using Conda is generally recommended when dealing with environments that require specific binary dependencies like PyTorch and CUDA.

1.  **Create and activate a new environment:**
    ```bash
    conda create -n nautilus python=3.11
    conda activate nautilus
    ```

2.  **Optional: Install PyTorch with CUDA if available** (or without CUDA if preferred).
    *(Choose the correct CUDA toolkit from the official PyTorch website.)*
    ```bash
    conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
    ```

3.  **Install the repository and development tools:**
    ```bash
    pip install -e .[dev]
    pre-commit install
    ```

4.  **Optional: Install JAX dependencies (for JAX-based algorithms):**
    This step installs the JAX backend (defaults to CPU). For GPU installation, refer to the official JAX documentation.
    ```bash
    pip install -e .[jax]
    ```

---

### Using Venv

You can also use the standard Python virtual environment (`venv`).

1.  **Create and activate environment:**
    ```bash
    python -m venv .venv
    source .venv/bin/activate Â  Â  Â  # Windows: .venv\Scripts\activate
    ```

2.  **Install dependencies and dev tools:**
    ```bash
    pip install -U pip
    pip install -e .[dev]
    pre-commit install
    ```

3.  **Optional: Install JAX dependencies (for JAX-based algorithms):**
    ```bash
    pip install -e .[jax]
    ```

---

Run your first PPO agent:

```bash
python nautilus/runners/ppo_runner.py --env-id CartPole-v1
```

See progress with TensorBoard:

```bash
tensorboard --logdir runs
```

Logs go to `runs/{env}__{seed}__{timestamp}/` and checkpoints to `checkpoints/{env}__{seed}__{timestamp}/`.

---

## ğŸ“‚ Repository structure

```
nautilus/
  core/         # buffers, networks, samplers, advantages
  algos/        # implementations (dqn/, ppo/, tabular/)
  envs/         # gym + dm-control wrappers
  utils/        # logging, seeding, config, checkpointing
  runners/      # train loops and CLI entrypoints
  configs/      # YAML configs per algorithm/env
  tests/        # pytest suites
scripts/        # runnable scripts (train_dqn.py, train_ppo.py)
notebooks/      # learning notebooks & experiments
```

---

## ğŸ“ˆ Logging & Tracking

- **TensorBoard**: run training in one terminal, then in another:

  ```bash
  python nautilus/runners/ppo_runner.py --env-id CartPole-v1
  tensorboard --logdir runs
  ```

- **Weights & Biases**: install `wandb` (`pip install wandb`) and set `WANDB_API_KEY`. Enable tracking with:

  ```bash
  python nautilus/runners/ppo_runner.py --env-id CartPole-v1 --track --wandb-project-name my-rl-experiments
  ```

  You can also pass `--wandb-entity <team>` to log to a shared org.
  If you prefer, run `wandb login` once instead of exporting `WANDB_API_KEY`.

Both TensorBoard and WandB logging are configured through `nautilus/utils/logger.py` and the runner flags.

---

## ğŸ§­ Learning roadmap

| Stage | Concepts | Implementation Targets |
|-------|-----------|------------------------|
| **M1 â€“ Foundations** | MDPs, returns, buffers, exploration | utils/, buffers/, samplers/, basic train loop |
| **M2 â€“ Bandits** | Îµ-greedy, UCB, regret | `algos/bandits/` |
| **M3 â€“ Tabular Q-learning** | DP vs TD, off-policy updates | `algos/tabular/q_learning.py` |
| **M4 â€“ Deep Q-Network (DQN)** | replay buffer, target net, Îµ-schedule | `algos/dqn/agent.py`, Atari wrappers |
| **M5 â€“ Policy Gradients â†’ PPO** | REINFORCE, GAE(Î»), clipping, entropy bonus | `algos/ppo/agent.py` |
| **M6 â€“ Extras** | Prioritized replay, n-step, distributed eval | `envs/`, `utils/`, `runners/` |

Each milestone comes with:
- Concept notebook (`notebooks/`)
- Unit tests (`tests/`)
- Reproducible configs (`configs/`)
- TensorBoard plots (`runs/`)

---

## ğŸ§ª Development

Lint, format, and test:

```bash
make lint
make test
```

Run pre-commit hooks manually:

```bash
pre-commit run --all-files
```

---

## âš™ï¸ Configuration

All hyperparameters and environment settings live in `configs/`, e.g.:

```yaml
# configs/algos/dqn/cartpole.yaml
seed: 1
env: CartPole-v1
steps: 50000
batch_size: 64
gamma: 0.99
lr: 0.001
sync_interval: 500
```

CLI overrides work out of the box:

```bash
python scripts/train_dqn.py --env CartPole-v1 --steps 100000
```

---

## ğŸ“’ Learning resources

These implementations are inspired by:
- *Understanding Deep Learning* â€” Simon Prince (Chapter 19)
- Sutton & Barto â€” *Reinforcement Learning: An Introduction*
- OpenAI Spinning Up and CleanRL

The idea is to **re-implement, not copy**, so each concept is fully understood and engineered cleanly.

---

## ğŸ§  Road to mastery

Once DQN and PPO are solid, weâ€™ll expand Nautilus to:
- Distributional & Dueling DQN, Noisy Nets
- SAC / TD3 for continuous control
- Multi-agent RL experiments
- LLM-driven agentic policy optimization
- Mixed-precision + MPI training

---

## ğŸ¤ Contributing

Contributions, questions, and refactors are welcome.
Open an issue or PR â€” especially for docs, configs, or new environments.

If youâ€™re learning RL: fork the repo, add your own experiments, and share results!

---

## ğŸª¶ License

MIT License Â© 2025 Alexander Braafladt

---

### ğŸŒŠ â€œBuild, test, understand â€” dive deeper.â€
